# 第１章
* 与えられたデータの数値をそのまま受け取るのではなく、その背後にある仕組みを考えることを<b>データのモデル化</b>という。
* 二条誤差の式は、その結果がそれらしいと判断できるための検算みたいなもの

## 機械学習の基礎的な作り方
1. 与えられたデータを元にして、位置のデータを予測する式を作る。
    <br/>今のところは、<br/>
    * シグモイド関数
    * マットマックス関数
2. パラメータの良し悪しを評価する関数<br/>
    * シグモイドで使った、確率の式
    * 最小二乗誤差はこれ
3. 誤差関数を最小にするようなパラメータの値を決定する
    いわゆるLoss関数がこれに当たる。

## ニューラルネットワークはサブルーチン
なんどもニューラルネットワークに通し、数式を通すことで、答えに行き着いている。
## より精密に結果を出力するには？
* 一層内のノードの数を経やす
* ノードの層を増やす<br/>
のいずれか。
しかし、トレードオフとして、<br/>
* パラメータの数が増えるため、パラメータ最適化の関数がうまく動作しない可能性がある

プーリング層は、画質を落とす。
画像の詳細をあえて消し去ることで、書かれている物体の本質的な部分を出そうとしている。

### recurrentニューラルネットワーク
自然言語処理に使われることもある。こういう人工知能の場合なら、マルコフ連鎖というものも、これに関連があるはず。

是非ともこちらでまとめておきたいものだ。

# 勾配降下法と勾配ベクトル
スカラのグラフの勾配を原点まで、持って行くに従って、勾配がきつくなればなるほど、勾配ベクトルが大きくなる。逆に、勾配が緩やかになると、ベクトルの長さは短くなる。これを<b>勾配降下法</b>という。


# 死ぬほど苦労していたデータの取り扱いについて基本からまとめてみよう。
## place holder
プレースホルダー。トレーニングセットのデータを保持する変数をいう。X

## Variable
最適化を実施するパラメーター。W

## 行列計算であることを頭に入れる。
この演算は行列演算である。そのため、行列の積のルールや、C#の配列の扱いなどが少し参考になるだろう。そこで忘れていけないのがブロードキャストルール。
### 行列計算のルールとして、大きさが合わない事象の時はエラーが出るので注意。

Placeholderと形が合わねぇぞ！みたいなことが書かれていたら、reshapeをかけて、配列の形を調整すること。

# train_stepは、勾配降下法などの探索のための関数。
train_step = Adamoptimizer　これは、勾配降下法のライブラリ。
.miniize(loss):誤差関数を使って、パラメータ修正を行う。

# sess = tf.session()の意味
プレースホルダ-にセッションを格納した

# sess.run(tf.global_variables_initializer())
これで、セッションを初期化する（クラスのインスタンスと感覚が似ている。）

# feed_dictの本当の意味
Placeholderに{}内の関数をぶち込んで行くという意味。なので、数や、要素の選択を間違えないようにすること。

<div style = "page-break-before:always"></div>

# 第2章　ロジスティック回帰

# シグモイド関数
σ(x) = 1/(1+exp(-x))

# pandas
pd.cancat([df1,df2])は配列を合成している。

# tf.mutmulについて
Xに含まれるデータと同じ個数の要素を持つベクトルになります。

# ブロードキャストルール
TensorFlowのみに適用される特別ルール。
1. 行列とスカラーの足し算は、各成分に対する足し算になる。
2. 同じサイズの行列の「＊」で掛け算した場合は、成分ごとの掛け算いなることを示します。
3. スカラーを受け取る関数を行列に適用すると、各成分で関数が適用される。

# tf.reducesum 
行列、多次元リスト＝＞ベクトルの拡張であるため、多次元リスト全ての要素を足し合わせるという処理を行う。

# tf.sign
ブール値を返す関数

# correct.prediction
トレーニングセットの各データに基づいて正解したかどうかのBool値を並べた縦ベクトル。

# シグモイド関数
シグモイド関数は別称、ロジスティック関数とも呼ばれており、ここで
